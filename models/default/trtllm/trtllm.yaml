# Complete TensorRT-LLM Serve Configuration Sample with DEFAULT VALUES
# This file contains all possible configuration options for trtllm-serve --extra_llm_api_options
# All values are set to their actual defaults from the TensorRT-LLM codebase
# You can safely use this file as-is without changing your existing behavior

# =============================================================================
# CORE SERVING PARAMETERS (Default Values)
# =============================================================================

# Basic batch and sequence limits - these are null by default, system will auto-determine
max_batch_size: 128                   # null = auto-determined from model/system
max_num_tokens: null                   # null = auto-determined from model/system  
max_seq_len: 48000                      # null = auto-determined from model config
max_beam_width: null                   # null = auto-determined, typically 1

# Model and tokenizer settings
trust_remote_code: false               # Allow execution of remote code from model repository
tokenizer_mode: auto                   # Tokenizer loading mode: "auto" or "slow"
skip_tokenizer_init: false             # Skip tokenizer initialization

# =============================================================================
# PARALLELISM CONFIGURATION (Default Values)
# =============================================================================

# Parallelism settings
tensor_parallel_size: 8                # Number of GPUs for tensor parallelism
pipeline_parallel_size: 1             # Number of GPUs for pipeline parallelism
context_parallel_size: 1               # Number of GPUs for context parallelism
enable_attention_dp: true             # Enable attention data parallelism

# =============================================================================
# KV CACHE CONFIGURATION (Default Values)
# =============================================================================

kv_cache_config:
  dtype: fp8
  enable_block_reuse: true             # Enable KV cache block reuse for better memory efficiency
  max_tokens: null                    # null = auto-determined from available memory
  max_attention_window: null           # null = no attention window limit
  sink_token_length: null              # null = no sink tokens
  free_gpu_memory_fraction: 0.7        # null = system auto-determines (typically ~0.9)
  host_cache_size: null                # null = auto-determined
  onboard_blocks: true                 # Enable onboard KV cache blocks
  cross_kv_cache_fraction: null        # null = no cross KV cache
  secondary_offload_min_priority: null # null = no secondary offload
  event_buffer_max_size: 0             # Maximum event buffer size
  enable_partial_reuse: true           # Enable partial block reuse
  copy_on_partial_reuse: true          # Copy data on partial reuse
  attention_dp_events_gather_period_ms: 5  # Event gathering period for attention DP

# =============================================================================
# SCHEDULER CONFIGURATION (Default Values)
# =============================================================================

scheduler_config:
  capacity_scheduler_policy: GUARANTEED_NO_EVICT  # Default scheduling policy
  context_chunking_policy: null        # null = no context chunking
  dynamic_batch_config: null           # null = no dynamic batching config

# =============================================================================
# CUDA GRAPH OPTIMIZATION (Default Values)
# =============================================================================

cuda_graph_config:
  batch_sizes: null
  max_batch_size: 128                    # 0 = CUDA graphs disabled
  enable_padding: true                # No padding to CUDA graph batch sizes

# =============================================================================
# BUILD CONFIGURATION (Default Values from BuildConfig)
# =============================================================================

build_config:
  max_input_len: 40000                  # Default maximum input length
  max_seq_len: 48000                   # null = auto-determined from model
  opt_batch_size: 32                    # Optimization batch size
  max_batch_size: 128                 # Default maximum batch size for build
  max_beam_width: 1                    # Default beam width for build
  max_num_tokens: 8192                 # Default maximum tokens for build

# =============================================================================
# QUANTIZATION CONFIGURATION (Default Values) (not usable with PyTorch)
# =============================================================================

quant_config:
  quant_algo: null                     # null = no quantization
  kv_cache_quant_algo: null            # null = no KV cache quantization
  group_size: 128                      # Group size for quantization
  smoothquant_val: 0.5                 # SmoothQuant alpha value
  clamp_val: null                      # null = no clamping
  use_meta_recipe: false               # Don't use Meta quantization recipe
  has_zero_point: false                # No zero point quantization
  pre_quant_scale: false               # No pre-quantized scale
  exclude_modules: null                # null = no modules excluded
  mamba_ssm_cache_dtype: null          # null = auto-determined

# =============================================================================
# CALIBRATION CONFIGURATION (Default Values) (not usable with PyTorch)
# =============================================================================

calib_config:
  device: cuda                         # Use CUDA for calibration
  calib_dataset: cnn_dailymail         # Default calibration dataset
  calib_batches: 512                   # Number of calibration batches
  calib_batch_size: 1                  # Calibration batch size
  calib_max_seq_length: 512            # Maximum sequence length for calibration
  random_seed: 1234                    # Random seed for calibration
  tokenizer_max_seq_length: 2048       # Tokenizer max sequence length

# =============================================================================
# MIXTURE OF EXPERTS (MoE) CONFIGURATION (Default Values)
# =============================================================================

moe_config:
  backend: TRTLLM                     # Default MoE backend
  max_num_tokens: null                 # null = no token limit for MoE
  load_balancer: null                  # null = no load balancer
  disable_finalize_fusion: false       # Enable FC2+finalize kernel fusion

# =============================================================================
# SPECULATIVE DECODING CONFIGURATION (Default Values)
# =============================================================================

# Note: speculative_config defaults vary by decoding_type, these are base defaults
speculative_config:
  decoding_type: MTP
  num_nextn_predict_layers: 3
  max_draft_len: null                  # null = no speculative decoding
  speculative_model_dir: null          # null = no speculative model
  max_concurrency: null                # null = no concurrency limit
  load_format: null                    # null = auto-determined

# For Lookahead decoding specifically (if enabled):
max_window_size: 5                   # Default lookahead window
max_ngram_size: 3                    # Default n-gram size  
max_verification_set_size: 4         # Default verification set size

# =============================================================================
# LORA CONFIGURATION (Default Values)
# =============================================================================

lora_config:
  lora_dir: []                         # Empty list = no LoRA directories
  lora_ckpt_source: hf                 # Default to HuggingFace format
  max_loras: 4                         # Maximum number of LoRA adapters
  max_lora_rank: 64                    # Maximum LoRA rank
  lora_target_modules: []              # Empty list = auto-determined targets
  max_cpu_loras: 4                     # Maximum CPU LoRA adapters
  optim_level: 0                       # No optimization by default

# =============================================================================
# PEFT (Parameter Efficient Fine-Tuning) CACHE CONFIGURATION (Default Values)
# =============================================================================

peft_cache_config:
  num_host_module_layer: 0             # No host module layers by default
  num_device_module_layer: 0           # No device module layers by default
  optimal_adapter_size: 8              # Optimal adapter size
  max_adapter_size: 64                 # Maximum adapter size
  num_put_workers: 1                   # Number of put workers
  num_ensure_workers: 1                # Number of ensure workers
  num_copy_streams: 1                  # Number of copy streams
  max_pages_per_block_host: 24         # Max pages per block on host
  max_pages_per_block_device: 8        # Max pages per block on device
  device_cache_percent: 0.02           # 2% device cache by default
  host_cache_size: 1073741824          # 1GB host cache size
  lora_prefetch_dir: null              # null = no prefetch directory

# =============================================================================
# ATTENTION DATA PARALLELISM CONFIGURATION (Default Values)
# =============================================================================

attention_dp_config:
  enable_balance: false                # No load balancing by default
  timeout_iters: 50                    # Default timeout iterations
  batching_wait_iters: 10              # Default batching wait iterations

# =============================================================================
# EXTENDED RUNTIME PERFORMANCE CONFIGURATION (Default Values) (not usable with PyTorch)
# =============================================================================

extended_runtime_perf_knob_config:
  multi_block_mode: true               # Enable multi-block mode by default
  enable_context_fmha_fp32_acc: false  # Disable FP32 accumulation in context FMHA
  cuda_graph_mode: false               # Disable CUDA graph mode by default
  cuda_graph_cache_size: 0             # No CUDA graph cache by default

# =============================================================================
# CACHE TRANSCEIVER CONFIGURATION (Default Values)
# =============================================================================

cache_transceiver_config:
  backend: null                        # null = no cache transceiver
  max_tokens_in_buffer: null           # null = auto-determined

# =============================================================================
# PERFORMANCE AND FEATURE FLAGS (Default Values)
# =============================================================================

# Performance optimizations
enable_chunked_prefill: true          # Disable chunked prefill by default
gather_generation_logits: false        # Don't gather generation logits by default
enable_lora: false                     # Disable LoRA support by default
num_postprocess_workers: 16             # No post-processing workers by default

# Guided decoding
guided_decoding_backend: null          # null = no guided decoding

# Statistics and monitoring
iter_stats_max_iterations: null        # null = no iteration statistics
request_stats_max_iterations: null     # null = no request statistics

# Error handling
fail_fast_on_attention_window_too_large: false  # Don't fail fast by default

# =============================================================================
# AUTODEPLOY BACKEND SPECIFIC OPTIONS (Not Default TRT-LLM Values)
# =============================================================================
# These are AutoDeploy-specific and only apply when using --backend _autodeploy

compile_backend: torch-opt           # AutoDeploy compilation backend
runtime: trtllm                      # AutoDeploy runtime engine  
skip_loading_weights: false          # Don't skip loading weights
free_mem_ratio: 0.8                  # Memory fraction for KV cache
cuda_graph_batch_sizes: [1, 2, 4, 8, 16, 32, 64]  # AutoDeploy CUDA graphs
attn_backend: flashinfer             # AutoDeploy attention backend

# =============================================================================
# BUILD CACHE CONFIGURATION (Default Values)
# =============================================================================

enable_build_cache: false              # Disable build cache by default

# =============================================================================
# ADDITIONAL CONFIGURATION (Default Values)
# =============================================================================

# Additional fields that can be overridden
tokenizer: null                        # null = auto-determined from model
dtype: auto                            # auto = determined from model config
revision: null                         # null = use default model revision
load_format: auto                      # auto = auto-detect format
batched_logits_processor: null         # null = no custom logits processor
